{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP24112 Summative Exercise: Air Quality Analysis (30 Marks)\n",
    "\n",
    "This lab exercise is about air quality analysis, where you will predict air quality through solving classification and regression tasks. You will submit a notebook file, a pdf report, and a trained model. You will be marked for implementation, design, result and analysis. Your code should be easy to read and your report should be concise (max 600 words). It is strongly recommended that you use a LaTeX editor, such as [Overleaf](https://www.overleaf.com/), to write your report.\n",
    "\n",
    "Please note your notebook should take no more than 10 minutes to run on lab computers. **There is 1 mark for code efficiency.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset and Knowledge Preparation\n",
    "\n",
    "The provided dataset contains measurements of air quality from a multisensor device. The device used spectrometer analyzers (variables marked by \"GT\") and solid state metal oxide detectors (variables marked by \"PT08.Sx\"), as well as temperature (T), relative humidity (RH) and absolute humidity (AH) sensors. \n",
    "\n",
    "The dataset contains 3304 instances of hourly averaged measurements taken at road level in a polluted city. You will predict the CO(GT) variable representing carbon monoxide levels. There are missing features in this dataset, flagged by the number `-999`. \n",
    "\n",
    "You will need to pre-process the dataset to handle missing features, for which please self-learn from scikit-learn on how to [impute missing values](https://scikit-learn.org/stable/modules/impute.html). You will need to split the dataset into training and testing sets, also to run cross validation, when you see fit. For this, please self-learn from scikit-learn on [data splitting](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>0.8</td>\n",
       "      <td>909.50</td>\n",
       "      <td>1.192106</td>\n",
       "      <td>517.50</td>\n",
       "      <td>65.8</td>\n",
       "      <td>1129.5</td>\n",
       "      <td>58.6</td>\n",
       "      <td>935.5</td>\n",
       "      <td>404.00</td>\n",
       "      <td>5.725</td>\n",
       "      <td>70.300001</td>\n",
       "      <td>0.649824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>1.7</td>\n",
       "      <td>1204.75</td>\n",
       "      <td>8.630947</td>\n",
       "      <td>924.75</td>\n",
       "      <td>132.0</td>\n",
       "      <td>922.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1546.5</td>\n",
       "      <td>1314.25</td>\n",
       "      <td>15.525</td>\n",
       "      <td>52.925000</td>\n",
       "      <td>0.927128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>0.6</td>\n",
       "      <td>692.25</td>\n",
       "      <td>1.513117</td>\n",
       "      <td>545.75</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1346.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>500.25</td>\n",
       "      <td>8.950</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>0.264819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.5</td>\n",
       "      <td>874.75</td>\n",
       "      <td>3.210646</td>\n",
       "      <td>665.00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1558.5</td>\n",
       "      <td>772.25</td>\n",
       "      <td>24.450</td>\n",
       "      <td>63.574999</td>\n",
       "      <td>1.921751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>2.7</td>\n",
       "      <td>1284.50</td>\n",
       "      <td>10.859535</td>\n",
       "      <td>1009.25</td>\n",
       "      <td>379.6</td>\n",
       "      <td>516.5</td>\n",
       "      <td>153.1</td>\n",
       "      <td>1452.0</td>\n",
       "      <td>1365.50</td>\n",
       "      <td>12.750</td>\n",
       "      <td>64.350001</td>\n",
       "      <td>0.944807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CO(GT)  PT08.S1(CO)   C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  \\\n",
       "1393     0.8       909.50   1.192106         517.50     65.8        1129.5   \n",
       "293      1.7      1204.75   8.630947         924.75    132.0         922.0   \n",
       "1306     0.6       692.25   1.513117         545.75     92.0        1346.5   \n",
       "192      0.5       874.75   3.210646         665.00     20.0         919.0   \n",
       "456      2.7      1284.50  10.859535        1009.25    379.6         516.5   \n",
       "\n",
       "      NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)       T         RH        AH  \n",
       "1393     58.6         935.5       404.00   5.725  70.300001  0.649824  \n",
       "293     107.0        1546.5      1314.25  15.525  52.925000  0.927128  \n",
       "1306     70.0         697.0       500.25   8.950  23.100000  0.264819  \n",
       "192      25.0        1558.5       772.25  24.450  63.574999  1.921751  \n",
       "456     153.1        1452.0      1365.50  12.750  64.350001  0.944807  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sklearn.model_selection\n",
    "\n",
    "notebook_start_time = time.time()\n",
    "\n",
    "# Import data - it should be saved in the same root directory as this notebook\n",
    "sensor_data_full = pd.read_excel('sensor_data.xlsx')\n",
    "\n",
    "# Display a sample of the data\n",
    "sensor_data_full.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Classification via Gradient Descent (13 marks)\n",
    "\n",
    "The air quality is assessed using the CO(GT) variable. If it is no greater than 4.5, the air quality is good (CO(GT)<=4.5), otherwise, it is bad  (CO(GT)>4.5). You will perform binary classification to predict whether the air quality is good based on the other 11 varivables, i.e., from PT08.S1(CO) to AH. \n",
    "\n",
    "### 2.1 Model Training and Testing (4 marks)\n",
    "\n",
    "This practice is about training a binary linear classifier by minimising a hinge loss with L2 (ridge) regularisation, and then testing its performance. Given a set of $N$ training samples $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$, where $\\mathbf{x}_i$ is the feature vector and $y_i \\in \\{-1, +1\\}$ is the class label for the $i$-th training sample, the training objective function to minimise is \n",
    "$$O = C \\sum^N_{i=1}\\max\\left(0, 1 - y_i \\left(\\mathbf{w}^T\\mathbf{x}_i + w_0\\right)\\right) + \\frac{1}{2}\\mathbf{w}^T\\mathbf{w}. $$\n",
    "Here, $\\mathbf{w}$ is a column weight vector of the linear model, $w_0$ is the bias parameter of the model, and $C$ is the regularisation hyperparameter.\n",
    "\n",
    "Recall from your lectures that gradient descent is an iterative optimisation algorithm typically used in model training. Complete the implmentation of the training function `linear_gd_train` below, which trains your linear model by minimising the above provided training objective function $O$ using gradient descent.\n",
    "\n",
    "The function should return the trained model weights and the corresponding objective function value (referred to as cost) per iteration. In addition to the training data, the function should take the regularisation hyperparameter $C$, learning rate $\\eta$, and the number of iterations $N_{max}$ as arguments. A default setting of these parameters has been provided below, which is able to provide reasonably good performance.  \n",
    "\n",
    "**Note that scikit-learn is not allowed for implementation in this section.** We recommend that you avoid using `for` loops in your implementation of the objective function or weight update, and instead use built-in numpy operations for efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gd_train(data, labels, c=0.2, n_iters=200, learning_rate=0.001, random_state=None # Add any other arguments here if needed\n",
    "          ):\n",
    "    \"\"\"\n",
    "    A summary of your function goes here.\n",
    "\n",
    "    data: training data\n",
    "    labels: training labels (boolean)\n",
    "    c: regularisation parameter\n",
    "    n_iters: number of iterations\n",
    "    learning_rate: learning rate for gradient descent\n",
    "\n",
    "    Returns an array of cost and model weights per iteration.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility if using random initialisation of weights (optional)\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "\n",
    "    # Create design matrix and labels\n",
    "    X_tilde = ...\n",
    "    y = ...\n",
    "\n",
    "    # Weight initialisation: use e.g. rng.standard_normal() or all zeros\n",
    "    w = ...\n",
    "\n",
    "    # Initialise arrays to store weights and cost at each iteration\n",
    "    w_all = ...\n",
    "    cost_all = ...\n",
    "    \n",
    "    # GD update of weights\n",
    "    for i in range(n_iters):\n",
    "        # Cost and gradient update of the linear model\n",
    "        cost = ...\n",
    "        \n",
    "        # Weight update\n",
    "        w = ...\n",
    "        \n",
    "        # save w and cost of each iteration in w_all and cost_all\n",
    "\n",
    "\n",
    "    # Return model parameters.\n",
    "    return cost_all, w_all\n",
    "\n",
    "\n",
    "def linear_predict(data, w):\n",
    "    \"\"\"\n",
    "    A summary of your function goes here.\n",
    "\n",
    "    data: test data\n",
    "    w: model weights\n",
    "\n",
    "    Returns the predicted labels.\n",
    "    \"\"\"\n",
    "\n",
    "    X_tilde = ...\n",
    "    y_pred = ...\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are ready to conduct a complete experiment of air quality classification. The provided code below splits the data into training and testing sets and imputes the missing features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Put a threshold on the labels to cast to binary: True if CO(GT) > 4.5, False otherwise\n",
    "binary_targets = (sensor_data_full['CO(GT)'] > 4.5).to_numpy()\n",
    "sensor_data = sensor_data_full.drop(columns=['CO(GT)']).to_numpy()\n",
    "\n",
    "# Named _cls to keep our classification experiments distinct from regression\n",
    "train_X_cls, test_X_cls, train_y_cls, test_y_cls = sklearn.model_selection.train_test_split(sensor_data, binary_targets, test_size=0.15, stratify=binary_targets)\n",
    "\n",
    "# Impute missing values and standardise the data\n",
    "imputer = SimpleImputer(missing_values=-999, strategy='mean')\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "train_X_cls = imputer.fit_transform(train_X_cls)\n",
    "train_X_cls = scaler.fit_transform(train_X_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your code below, which should train the model, plot the training objective function value and the classification accuracy of the training set over iterations, and print the classification accuracy and $F_1$ score of the testing set. Note, use the default setting provided for $C$, $\\eta$ and $N_{max}$. Your plot should have axis labels and titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Plot accuracy and cost per iteration on training set\n",
    "\n",
    "# Apply imputation to the test set\n",
    "\n",
    "# Predict on test set, report accuracy and f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Learning Rate Analysis (3 marks)\n",
    "\n",
    "The learning rate $\\eta$ (Greek letter \"eta\") is a key parameter that affects the model training and performance. Design an appropriate experiment to demonstrate the effect of $\\eta$ on model training, and on the model performance during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Report (6 Marks)\n",
    "Answer the following questions in your report, to be submitted separately:\n",
    "1. Derive step-by-step the gradient of the provided training objective function $O$, and the updating equation of your model weights based on gradient descent. (3 marks)\n",
    "\n",
    "2. What does the figure from section 2.1 tell you, and what is the indication of the classification accuracies of your training and testing sets? (1 mark)\n",
    "\n",
    "3. Comment on the effect of $\\eta$ on model training, and on the model performance during testing, based on your results observed in Section 2.2. (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Air Quality Analysis by Neural Network (10 marks)\n",
    "\n",
    "In this experiment, you will predict the CO(GT) value based on the other 11 variables through regression. You will use a neural network to build a nonlinear regression model. Familiarise yourself with how to build a regression model by mutlilayer perceptron (MLP) using the scikit learn tutorial (https://scikit-learn.org/stable/modules/neural_networks_supervised.html#regression). \n",
    "\n",
    "\n",
    "### 3.1 Simple MLP Model Selection (4 marks)\n",
    "\n",
    "This section is focused on the practical aspects of MLP implementation and model selection. We will first compare some model architectures. \n",
    "\n",
    "The set of MLP architectures to select is specified in `param_grid` below, including two MLPs with one hidden layer, where one has a small number of 3 hidden neurons, while the other has a larger number of 100 hidden neurons, and two MLPs with two hidden layers, where one is small (3, 3) and the other is larger (100, 100). It also includes two activation function options, i.e., the logistic and the rectified linear unit (\"relu\").  These result in a total of 8 model options, where sklearn default parameters are used for all the MLPs and their training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {   \n",
    "        'hidden_layer_sizes': [(3,), (100,), (3, 3), (100, 100)],\n",
    "        'activation': ['relu', 'logistic'],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code below should do the following: Split the dataset into the training and testing sets. Preprocess the data by imputing the missing features. Use the training set for model selection by cross-validation, and use mean squared error (MSE) as the model selection performance metric. You can use the scikit-learn module [GridSearchCV](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) to conduct grid search. Print the cross-validation MSE with standard deviation of the selected model. Re-train the selected model using the whole training set, and print its MSE and $R^2$ score for the testing set.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo split for regression\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "\n",
    "\n",
    "# Define MLP model\n",
    "\n",
    "\n",
    "# Initialise and fit the grid search\n",
    "\n",
    "\n",
    "# Report the best parameters and the CV results\n",
    "\n",
    "\n",
    "# Report model performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training Algorithm Comparison: SGD and ADAM (2 Marks)\n",
    "\n",
    "In this exercise, you will compare two training algorithms, stochastic gradient descent (SGD) and ADAM optimisation, for training an MLP with two hidden layers each containing 100 neurons with \"relu\" activation, under the settings specified in `test_params` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = [\n",
    "    {\n",
    "        'activation': 'relu', \n",
    "        'alpha': 0.001, \n",
    "        'early_stopping': False, \n",
    "        'hidden_layer_sizes': (100, 100), \n",
    "        'solver': 'adam'\n",
    "    },{\n",
    "        'activation': 'relu', \n",
    "        'alpha': 0.001, \n",
    "        'early_stopping': False, \n",
    "        'hidden_layer_sizes': (100, 100), \n",
    "        'learning_rate': 'adaptive', \n",
    "        'momentum': 0.95, \n",
    "        'solver': 'sgd'\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code below, where each training algorithm should run for 300 iterations (make sure to set `early_stopping=False`). For both algorithms, (1) plot the training loss (use the defaul loss setting in sklearn), as well as the MSE of both training and testing sets, over iterations; and (2) print the MSE and $R^2$ score of the trained model using the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models and plot learning curves\n",
    "\n",
    "\n",
    "# Print final test set performance for both models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Report (4 Marks)\n",
    "Answer the following questions in your report, to be submitted separately:\n",
    "1. What conclusions can you draw based on your model selection results in Section 3.1? (2 marks)\n",
    "\n",
    "2. Comment on the two training algorithms based on your results obtained in Section 3.2. (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Build A Robust MLP Regressor (6 Marks)\n",
    "\n",
    "In this last experiment, you will develop and submit a robust MLP regressor to predict the CO(GT) value based on the other 11 variables, using the provided dataset. This robust regressor should account for the presence of missing and noisy features. \n",
    "\n",
    "Once you have developed your model, save it to a file using the provided `save_model` function for submission.\n",
    "\n",
    "### 4.1 Model Development (3 Marks)\n",
    "\n",
    "What you consider in model development should include (but not limited to) (1) handling of missing features in the unseen testing data, (2) handling of noisy features in the unseen testing data, and (3) a model selection practice.\n",
    "\n",
    "Write your model development code below. Describe briefly in your report what you have considered in your model development.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### OPTIONAL: move completed model selection code to this 'raw' cell below before final submission \n",
    "# You can run your code once to select the best hyperparameters and train your model, then\n",
    "# move the code to this cell for submission. This prevents it from executing automatically and so\n",
    "# excludes time-consuming hyperparameter selection from total notebook run time during marking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 External Testing (3 Marks)\n",
    "Save your trained model for submission, and submit it along with your notebook and report. It will be run and evaluated on a test set unseen by you.\n",
    "\n",
    "**Important: set your university username (e.g. mbxxabc3) below when saving your model.** Failure to do this correctly would lead to your model not being marked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_eval_utils\n",
    "\n",
    "#### SAVE YOUR MODEL\n",
    "student_username = \"mbxxxxx0\" # SET YOUR USERNAME HERE\n",
    "model = ... # SET YOUR MODEL HERE\n",
    "model_eval_utils.save_model(student_username, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total notebook run time: 339 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total notebook run time: {time.time() - notebook_start_time:.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option to test your saved model\n",
    "Use the `run_model()` function to make sure your saved model can be loaded and run before submitting.\n",
    "\n",
    "Please note the score returned by `run_model()` is not in any way indicative of your final mark. This is just a simple test to make sure your model can be loaded and run. When testing your model, the GTA will run your model following the practice below, but replacing the bunk_data with the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some bunk data\n",
    "bunk_data = np.array([[ 1.22400000e+03,  9.97440117e+00,\n",
    "        9.76666667e+02,  2.50600000e+02,  5.71333333e+02,\n",
    "        1.30700000e+02,  1.42433333e+03,  1.00166667e+03,\n",
    "        2.32666664e+01,  3.57999992e+01,  1.00855762e+00],\n",
    "    [ 9.24250000e+02,   3.97337806e+00,\n",
    "        7.09250000e+02,  6.30000000e+01,  1.15800000e+03,\n",
    "        7.50000000e+01,  1.31800000e+03,  6.09750000e+02,\n",
    "        1.60500000e+01,  4.13500004e+01,  7.48680717e-01],\n",
    "    [ 8.92000000e+02,  5.06611560e+00,\n",
    "        7.66750000e+02,  7.10000000e+01,  1.18000000e+03,\n",
    "        8.40000000e+01,  1.46600000e+03,  6.56750000e+02,\n",
    "        1.79749999e+01,  5.14499998e+01,  1.05039283e+00]])\n",
    "bunk_labels = np.array([2. , 1.3, 6.1])\n",
    "\n",
    "score = model_eval_utils.run_model(student_username, \n",
    "                                test_data=bunk_data, \n",
    "                                test_labels=bunk_labels, \n",
    "                                model_folder=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
